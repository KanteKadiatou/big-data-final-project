# big-data-final-project
Projet Master 1 DIT 2025

# Plateforme Big Data pour lâ€™analyse des comportements dâ€™apprentissage en ligne

## ğŸ¯ Objectif

Ce projet met en place une plateforme Big Data complÃ¨te permettant de **collecter**, **stocker**, **traiter**, **analyser** et **visualiser** les comportements dâ€™apprentissage en ligne Ã  partir de donnÃ©es rÃ©elles (YouTube, Kaggle) et simulÃ©es.

Lâ€™objectif est dâ€™extraire des **KPIs dâ€™engagement**, de suivi et de performance des apprenants.

---

# ğŸ—ï¸ Architecture globale du projet

```
BIG-DATA-FINAL-PROJECT
â”‚
â”œâ”€â”€ airflow_dags/               â†’ Pipelines Airflow (DAGs)
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                    â†’ DonnÃ©es brutes (Kaggle, YouTube, donnÃ©es simulÃ©es)
â”‚   â”‚   â”œâ”€â”€ kaggle/
â”‚   â”‚   â”œâ”€â”€ simulated/
â”‚   â”‚   â””â”€â”€ youtube/
â”‚   â”œâ”€â”€ processed/              â†’ DonnÃ©es nettoyÃ©es / enrichies
â”‚   â””â”€â”€ curated/                â†’ DonnÃ©es finales prÃªtes pour lâ€™analyse
â”‚
â”œâ”€â”€ infrastructure/
â”‚   â””â”€â”€ minio/
â”‚       â”œâ”€â”€ docker-compose.yml  â†’ Lancement du serveur MinIO
â”‚       â””â”€â”€ minio_data/         â†’ RÃ©pertoires utilisÃ©s comme Data Lake MinIO
â”‚           â”œâ”€â”€ datalake-raw
â”‚           â”œâ”€â”€ datalake-clean
â”‚           â”œâ”€â”€ datalake-curated
â”‚           â””â”€â”€ processed
â”‚
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ exploration.ipynb       â†’ Analyse exploratoire et tests
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ collectors/             â†’ Scripts dâ€™ingestion (YouTube API, Kaggle, Faker)
â”‚   â”œâ”€â”€ processing/             â†’ Nettoyage, transformation (PySpark)
â”‚   â”œâ”€â”€ analytics/              â†’ KPIs, agrÃ©gations, modÃ¨les
â”‚   â””â”€â”€ ingestion.py            â†’ Script principal dâ€™ingestion
â”‚
â””â”€â”€ README.md
```

---

# ğŸ“¥ Phase 1 â€” Collecte des donnÃ©es

Sources intÃ©grÃ©es :

### âœ” Kaggle (datasets publics)

* `students_adaptability_level_online_education.csv`
* `StudentsPerformance.csv`

### âœ” Simulations (Faker)

* `students_synthetic.csv`

### âœ” API YouTube

Collecte de mÃ©tadonnÃ©es (vues, likes, durÃ©e, titres...) dans :

```
data/raw/youtube/videos.json
```

Scripts situÃ©s dans :
`src/collectors/`

---

# ğŸ—„ï¸ Phase 2 â€” Stockage (Data Lake MinIO)

Le Data Lake suit une organisation industrielle en zones :

```
minio_data/
â”œâ”€â”€ datalake-raw/       â†’ DonnÃ©es brutes
â”œâ”€â”€ datalake-clean/     â†’ DonnÃ©es nettoyÃ©es
â”œâ”€â”€ datalake-curated/   â†’ DonnÃ©es finales
â””â”€â”€ processed/          â†’ Transformations intermÃ©diaires
```

Lancement de MinIO :

```
cd infrastructure/minio
docker-compose up -d
```

---

# âš™ï¸ Phase 3 â€” Traitement & Analyse (PySpark)

Les pipelines rÃ©alisent :

* Nettoyage (nulls, formats, colonnes inutiles)
* Normalisation des jeux de donnÃ©es
* Fusion multi-sources
* Calcul dâ€™indicateurs pÃ©dagogiques

Principaux KPIs extraits :

* Taux dâ€™engagement
* Taux de complÃ©tion
* Temps passÃ©
* RÃ©tention
* PopularitÃ© des cours
* ActivitÃ© temporelle

Scripts situÃ©s dans :
`src/processing/` et `src/analytics/`

---

# ğŸ”„ Phase 4 â€” Orchestration (Apache Airflow)

Automatisation des workflows via :

```
airflow_dags/
```

DAGs inclus :

* ingestion quotidienne des donnÃ©es
* nettoyage PySpark
* calcul des KPIs
* dÃ©pÃ´t dans `/curated`

---

# ğŸ“Š Phase 5 â€” Visualisation (Streamlit)

Dashboard interactif affichant :

* Taux dâ€™engagement
* Taux de complÃ©tion
* ActivitÃ© horaire/journaliÃ¨re
* Cours les plus suivis
* Comparaison Kaggle vs SimulÃ© vs YouTube

Lancement :

```
streamlit run dashboard/app.py
```

---

# â–¶ï¸ Installation

### 1. Cloner le projet

```
git clone https://github.com/KanteKadiatou/big-data-final-project.git
cd BIG-DATA-FINAL-PROJECT
```

### 2. Installer les dÃ©pendances

```
pip install -r requirements.txt
```

### 3. Lancer MinIO (facultatif mais recommandÃ©)

```
cd infrastructure/minio
docker-compose up -d
```

### 4. Lancer les notebooks

Ouvrir :

```
notebooks/exploration.ipynb
```

---

# ğŸ› ï¸ Technologies utilisÃ©es

* Python 3.10+
* PySpark
* Apache Airflow
* Streamlit
* MinIO (S3 compatible)
* Pandas / NumPy
* YouTube Data API
* Faker
* Docker / Docker Compose

---

# ğŸ‘¤ Auteur

Projet rÃ©alisÃ© par **[Kadiatou KantÃ© / Mouctar Diallo]** â€” 2025.
